{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  851\n",
      "Number of validation images:  106\n",
      "Number of test images:  108\n"
     ]
    }
   ],
   "source": [
    "# original data directories\n",
    "mypaths = ['../Data/CT', '../Data/MRI', '../Data/PET']\n",
    "\n",
    "# new data directories\n",
    "folders = ['data/train', 'data/validation', 'data/test']\n",
    "\n",
    "def build_dataset(datadir, folders_names):\n",
    "    # create folders if not already there\n",
    "    for folder in folders:\n",
    "        if os.path.isdir(folder) == False:\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # create database\n",
    "    database = [0,0,0] # train,val,test\n",
    "    for path in mypaths:\n",
    "\n",
    "        # gathering file names from original Data directory\n",
    "        filenames = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        filenames.sort()\n",
    "        random.seed(432)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "        # splitting 80 10 10\n",
    "        split_validation = int(0.8 * len(filenames)) \n",
    "        split_test = int(0.9 * len(filenames)) \n",
    "        train_filenames = filenames[:split_validation]\n",
    "        validation_filenames = filenames[split_validation:split_test]\n",
    "        test_filenames = filenames[split_test:]\n",
    "\n",
    "        # copy files in data directory\n",
    "        for f in train_filenames:\n",
    "            shutil.copyfile(path+'/'+f, 'data/train/'+f)\n",
    "\n",
    "        for f in validation_filenames:\n",
    "            shutil.copyfile(path+'/'+f, 'data/validation/'+f)\n",
    "\n",
    "        for f in test_filenames:\n",
    "            shutil.copyfile(path+'/'+f, 'data/test/'+f)\n",
    "        \n",
    "       \n",
    "        # count number of images per directory\n",
    "        database[0] += len(train_filenames)        \n",
    "        database[1] += len(validation_filenames)\n",
    "        database[2] += len(test_filenames)\n",
    "\n",
    "    print('Number of training images: ', database[0])        \n",
    "    print('Number of validation images: ', database[1])\n",
    "    print('Number of test images: ', database[2])\n",
    "\n",
    "build_dataset(mypaths, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-7464207f4b92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/image.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "train_transforms = transforms.Compose([#transforms.Resize(128), \n",
    "                                       transforms.ToTensor()])\n",
    "train_data = datasets.ImageFolder(folders[0], transform=train_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16)\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread('/path/to/image.png')\n",
    "\n",
    "dimensions = img.shape\n",
    "\n",
    "# PROBLEMA DA RISOLVERE: LE IMMAGINI CHE HO SONO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
