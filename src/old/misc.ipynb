{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init \n",
    "#train_loss = [0,0,0]\n",
    "#val_loss = [0,0,0]\n",
    "index = 0\n",
    "train_index_array = np.arange(x_train.shape[0])\n",
    "#val_index_array = np.arange(x_val.shape[0])\n",
    "\n",
    "# plots\n",
    "history_train_losses = [[],[],[]]\n",
    "#history_val_losses = [[],[],[]]\n",
    "history_train_acc=[]\n",
    "#history_val_acc=[]\n",
    "iterazioni=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "\n",
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q, _ = model.predict(x_train, verbose=0)\n",
    "        #qval, _ = model.predict(x_val, verbose=0)\n",
    "        p = target_distribution(q) # update the auxiliary target distribution p\n",
    "        #pval = target_distribution(qval)\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_train_pred = q.argmax(1)\n",
    "        #y_val_pred = qval.argmax(1)\n",
    "        if y_train is not None:\n",
    "            train_acc = np.round(metrics.acc(y_train, y_train_pred), 5)\n",
    "            train_nmi = np.round(metrics.nmi(y_train, y_train_pred), 5)\n",
    "            train_ari = np.round(metrics.ari(y_train, y_train_pred), 5)\n",
    "            train_loss = np.round(train_loss, 5)\n",
    "            print('Iter', ite, ': Acc tr', train_acc, ', nmi tr', train_nmi, ', ari tr', train_ari, '; loss tr=', train_loss)\n",
    "        #if y_val is not None:\n",
    "        #    val_acc = np.round(metrics.acc(y_val, y_val_pred), 5)\n",
    "        #    val_nmi = np.round(metrics.nmi(y_val, y_val_pred), 5)\n",
    "        #    val_ari = np.round(metrics.ari(y_val, y_val_pred), 5)\n",
    "        #    val_loss = np.round(val_loss, 5)\n",
    "        #    print('Iter', ite, ': Acc val', val_acc, ', nmi tr', val_nmi, ', ari tr', val_ari, '; val_loss=', val_loss) \n",
    "            \n",
    "        # check stop criterion\n",
    "        delta_label = np.sum(y_train_pred != y_pred_last).astype(np.float32) / y_train_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_train_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    \n",
    "    # train on batch\n",
    "    idx_train = train_index_array[index * batch_size: min((index+1) * batch_size, x_train.shape[0])]\n",
    "\n",
    "    train_loss = model.train_on_batch(x=x_train[idx_train], \n",
    "                                      y=[p[idx_train], x_train[idx_train]])\n",
    "    history_train_losses[0].append(train_loss[0])\n",
    "    history_train_losses[1].append(train_loss[1])\n",
    "    history_train_losses[2].append(train_loss[2])\n",
    "    history_acc.append(train_acc)\n",
    "\n",
    "    #idx_val = val_index_array[index * batch_size: min((index+1) * batch_size, x_val.shape[0])]\n",
    "    #val_loss = model.test_on_batch(x=x_val[idx_val], \n",
    "    #                               y=[pval[idx_val], x_val[idx_val]])\n",
    "    #history_val_losses[0].append(val_loss[0])\n",
    "    #history_val_losses[1].append(val_loss[1])\n",
    "    #history_val_losses[2].append(val_loss[2])\n",
    "    #history_acc.append(val_acc)\n",
    "    \n",
    "    index = index + 1 if (index + 1) * batch_size <= x_train.shape[0] else 0\n",
    "    \n",
    "    iterazione.append(ite)\n",
    "    \n",
    "    if ite % save_interval == 0:\n",
    "        # save DCEC model checkpoints\n",
    "        print('saving model to:', save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "        model.save_weights(save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "\n",
    "# save the trained model\n",
    "print('saving model to:', save_dir + '/dcec_model_final.h5')\n",
    "model.save_weights(save_dir + '/dcec_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    input_img = Input(shape=(dims[0],), name='input')\n",
    "    x = input_img\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function for reshaping arrays for DenseAutoEncoder\n",
    "def ReshapeDAE(array):\n",
    "    array = array.reshape(array.shape[0], -1)\n",
    "    array = array/255.\n",
    "    return array\n",
    "\n",
    "#scegli la funzione\n",
    "f = ReshapeDAE\n",
    "\n",
    "x_train = f(x_train)\n",
    "x_val = f(x_val)\n",
    "x_test = f(x_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "dims = [x.shape[-1], 500, 500, 2000, 3]\n",
    "print(dims[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS THE AUTOENCODER CON IL DENSE PRIMA DELL'EMBEDDED E DOPO L'EMBEDDED: il train ha data come risultati delle immagini schifo (vedi risultati_autoencoder_schifo.png)\n",
    "\n",
    "\n",
    "\n",
    "def autoencoderConv2D_1(input_shape=(128, 128, 1), filters=[32, 64, 128, 256, 3]):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    if input_shape[0] % 8 == 0:\n",
    "        pad3 = 'same'\n",
    "    else:\n",
    "        pad3 = 'valid'\n",
    "    x = Conv2D(filters[0], 5, strides=2, padding='same', activation='relu', name='conv1', input_shape=input_shape)(input_img)\n",
    "\n",
    "    x = Conv2D(filters[1], 5, strides=2, padding='same', activation='relu', name='conv2')(x)\n",
    "\n",
    "    x = Conv2D(filters[2], 3, strides=2, padding=pad3, activation='relu', name='conv3')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(units=filters[3])(x) ##\n",
    "    \n",
    "    encoded = Dense(units=filters[4], name='embedding')(x)\n",
    "    \n",
    "    x = Dense(units=filters[3], activation='relu')(encoded)\n",
    "    \n",
    "    x = Dense(units=filters[2]*int(input_shape[0]/8)*int(input_shape[0]/8), activation='relu')(x) ##\n",
    "\n",
    "    x = Reshape((int(input_shape[0]/8), int(input_shape[0]/8), filters[2]))(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters[1], 3, strides=2, padding=pad3, activation='relu', name='deconv3')(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters[0], 5, strides=2, padding='same', activation='relu', name='deconv2')(x)\n",
    "\n",
    "    decoded = Conv2DTranspose(input_shape[2], 5, strides=2, padding='same', name='deconv1')(x)\n",
    "    \n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv2DTranspose\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoderConv2D_1(input_shape=(128, 128, 1), filters=[32, 64, 128, 256, 3]):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    if input_shape[0] % 8 == 0:\n",
    "        pad3 = 'same'\n",
    "    else:\n",
    "        pad3 = 'valid'\n",
    "    x = Conv2D(filters[0], 5, strides=2, padding='same', activation='relu', name='conv1', input_shape=input_shape)(input_img)\n",
    "\n",
    "    x = Conv2D(filters[1], 5, strides=2, padding='same', activation='relu', name='conv2')(x)\n",
    "\n",
    "    x = Conv2D(filters[2], 3, strides=2, padding=pad3, activation='relu', name='conv3')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    encoded = Dense(units=filters[3], name='embedding')(x)\n",
    "    \n",
    "    y = Dense(units=filters[4])(encoded)\n",
    "    \n",
    "    x = Dense(units=filters[2]*int(input_shape[0]/8)*int(input_shape[0]/8), activation='relu')(encoded)\n",
    "\n",
    "    x = Reshape((int(input_shape[0]/8), int(input_shape[0]/8), filters[2]))(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters[1], 3, strides=2, padding=pad3, activation='relu', name='deconv3')(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters[0], 5, strides=2, padding='same', activation='relu', name='deconv2')(x)\n",
    "\n",
    "    decoded = Conv2DTranspose(input_shape[2], 5, strides=2, padding='same', name='deconv1')(x)\n",
    "    \n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=y, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 64, 64, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 32, 32, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 256)               8388864   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 8,515,587\n",
      "Trainable params: 8,515,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "AUTOENCODER\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 64, 64, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 32, 32, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 256)               8388864   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32768)             8421376   \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)    (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)    (None, 64, 64, 32)        51232     \n",
      "_________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)    (None, 128, 128, 1)       801       \n",
      "=================================================================\n",
      "Total params: 17,062,017\n",
      "Trainable params: 17,062,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = autoencoderConv2D_1()\n",
    "\n",
    "print('ENCODER')\n",
    "encoder.summary()\n",
    "\n",
    "print('AUTOENCODER')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Category10_10\n",
    "from bokeh.resources import INLINE\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook(INLINE) \n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "def embeddable_image(data):\n",
    "    data = data[:,:,0]\n",
    "    buffer = BytesIO()\n",
    "    plt.imsave(buffer, data, cmap='gray')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "emb = model.predict(x_test)[0]\n",
    "digits_df = pd.DataFrame(emb, columns=('x', 'y','z'))\n",
    "digits_df['labels'] = [str(i) for i in y_test]\n",
    "digits_df['image'] = list(map(embeddable_image, list(x_test)))\n",
    "#digits_df['image_name'] = [dset_train.x_name[i] for i in range(len(dset_train))]\n",
    "digits_df = digits_df\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "color_mapping = CategoricalColorMapper(factors=['0', '1', '2'],\n",
    "                                       palette=Category10_10)\n",
    "plot_figure = figure(\n",
    "    plot_width=800,\n",
    "    plot_height=800,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 1px 1px 1px 1px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 13px'>@labels</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=ColumnDataSource(digits_df),\n",
    "    color=dict(field='labels', transform=color_mapping)\n",
    "    )\n",
    "plot_figure.cross(\n",
    "    x=centers[:,0],\n",
    "    y=centers[:,1],\n",
    "    size=20,\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "#plot_figure.legend.location = \"top_left\"\n",
    "#plot_figure.legend.click_policy=\"hide\"\n",
    "show(plot_figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
